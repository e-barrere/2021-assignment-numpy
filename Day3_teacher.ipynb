{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DAY 3: Teacher version"
      ],
      "metadata": {
        "id": "Xycip-y7MmUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning NLP"
      ],
      "metadata": {
        "id": "gnCKD6NtjZno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "TBamWPKlkZs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from scipy.sparse import find\n"
      ],
      "metadata": {
        "id": "50KuGjmPjquG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "bVl7AEcBkgyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# MAIN_PATH = '/content/drive/MyDrive/TP Centrale'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVb0xZIvkevD",
        "outputId": "ba315e74-98f6-40c8-f5eb-f59f8fbce9b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install py7zr --quiet"
      ],
      "metadata": {
        "id": "A-RTshiBofj4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import py7zr\n",
        "# archive = py7zr.SevenZipFile(os.path.join(MAIN_PATH, 'datascience.stackexchange.com.7z'), mode='r')\n",
        "# archive.extractall(path=os.path.join(MAIN_PATH, 'data'))\n",
        "# archive.close()"
      ],
      "metadata": {
        "id": "o5nq9vXaolpD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/drive/MyDrive/TP Centrale/data/'"
      ],
      "metadata": {
        "id": "46RvgDwupZcP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tags = pd.read_xml(DATA_PATH + 'Tags.xml')"
      ],
      "metadata": {
        "id": "gK8zD-8_phcs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts = pd.read_xml('/content/drive/MyDrive/TP Centrale/data/Posts.xml', parser = 'etree')"
      ],
      "metadata": {
        "id": "s-3v0BQyppCJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts = posts.loc[:1000]"
      ],
      "metadata": {
        "id": "K6U2GMc-0rpI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts = posts[~posts['Body'].isna()]"
      ],
      "metadata": {
        "id": "aJc1xwA71GC_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts.Body.values[15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "Ln2drob8qm0C",
        "outputId": "a3c969dc-6ac4-4e36-e3c1-29b5b9f90fff"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<p>A few gigabytes is not very \"<strong>big</strong>\". It\\'s more like the normal size of an enterprise DB. As long as you go over PK when joining tables it should work out really well, even in the future (as long as you don\\'t get TB\\'s of data a day).</p>\\n\\n<p>Most professionals working in a big data environment consider <strong>> ~5TB</strong> as the <em>beginning</em> of the term big data. But even then it\\'s not always the best way to just install the next best nosql database. You should always think about the task that you want to archive with the data (aggregate,read,search,mine,..) to find the best tools for you problem.</p>\\n\\n<p>i.e. if you do alot of searches in you database it would probably be better to run a solr instance/cluster and denormalize your data from a DBMS like Postgres or your SQL Server from time to time and put it into solr instead of just moving the data from sql to nosql in term of persistence and performance.</p>\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning"
      ],
      "metadata": {
        "id": "2q_ryLonxLYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_post(post):\n",
        "    # remove html, including the data inside\n",
        "    # lower text \n",
        "    post = post.lower()\n",
        "\n",
        "    pattern_html = re.compile('<.*?>')\n",
        "    clean_post = pattern_html.sub('', post)\n",
        "\n",
        "    # remove other things\n",
        "\n",
        "    clean_post = clean_post.replace('\\n', '').replace('\\t', '')\n",
        "    return clean_post\n"
      ],
      "metadata": {
        "id": "9e-XxHMAkmEu"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_post(posts.Body.values[15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "GfyliBv8vyhV",
        "outputId": "d05ceeb7-234b-434e-ffe6-f8d8d01ccd25"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a few gigabytes is not very \"big\". it\\'s more like the normal size of an enterprise db. as long as you go over pk when joining tables it should work out really well, even in the future (as long as you don\\'t get tb\\'s of data a day).most professionals working in a big data environment consider > ~5tb as the beginning of the term big data. but even then it\\'s not always the best way to just install the next best nosql database. you should always think about the task that you want to archive with the data (aggregate,read,search,mine,..) to find the best tools for you problem.i.e. if you do alot of searches in you database it would probably be better to run a solr instance/cluster and denormalize your data from a dbms like postgres or your sql server from time to time and put it into solr instead of just moving the data from sql to nosql in term of persistence and performance.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "posts['cleaned_body'] = posts.Body.apply(clean_post)"
      ],
      "metadata": {
        "id": "Yv_n2e5Y1a04"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_entry(entry):\n",
        "    # a am√©liorer si besoin\n",
        "    # voir si vraiment besoin aussi\n",
        "    return entry.lower()"
      ],
      "metadata": {
        "id": "Q8_gvmjXwBwt"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classic Preprocessing"
      ],
      "metadata": {
        "id": "o0rsRxD4xeli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal for this part is to implement a classic vectorization.\n",
        "\n",
        "Hints : pay attention to stopwords, additionnal preprocessing steps and techniques of vectoriation\n"
      ],
      "metadata": {
        "id": "NLmg2T3qxjc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stopwords removal (on utlise la list de sklearn car c'est de l'anglais)\n",
        "# lemmatizing\n",
        "# tfidf or count vectorizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk import word_tokenize          \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "class LemmaTokenizer(object):\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "    def __call__(self, articles):\n",
        "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
        "\n",
        "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(),\n",
        "                                strip_accents = 'unicode', # works \n",
        "                                stop_words = 'english', # works\n",
        "                                lowercase = True, # works\n",
        "                                # binary=True, # we use binary to indicate the precense of word, not the frequency (debatable)\n",
        "                                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0rtLjshxeOl",
        "outputId": "57e78632-4acd-436d-b542-0df2d157f26f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.fit(posts.cleaned_body.values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "WGj3RHjiz5_a",
        "outputId": "6bd25680-5a7e-4834-b101-ecf24e1d9cb0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(stop_words='english', strip_accents='unicode',\n",
              "                tokenizer=<__main__.LemmaTokenizer object at 0x7f4e26988460>)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(stop_words=&#x27;english&#x27;, strip_accents=&#x27;unicode&#x27;,\n",
              "                tokenizer=&lt;__main__.LemmaTokenizer object at 0x7f4e26988460&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(stop_words=&#x27;english&#x27;, strip_accents=&#x27;unicode&#x27;,\n",
              "                tokenizer=&lt;__main__.LemmaTokenizer object at 0x7f4e26988460&gt;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = vectorizer.transform(posts.cleaned_body.values)"
      ],
      "metadata": {
        "id": "RjqRSPgW0GAi"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function that applies the same process to the entry"
      ],
      "metadata": {
        "id": "drg1x_9v1RD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_entry(entry):\n",
        "    cleaned_entry = clean_entry(entry)\n",
        "    return vectorizer.transform([cleaned_entry])"
      ],
      "metadata": {
        "id": "RpBr0mfF1QVr"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine a way to use this vectorization to suggest the closest items to the entry in the database"
      ],
      "metadata": {
        "id": "ZO5uBYas15HS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# possible to think about different ways : cosine distance, sum of common words, play with the count vectorizer or the tfidf properties "
      ],
      "metadata": {
        "id": "d0He3wCu14wi"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorizer_search(entry):\n",
        "    vector_entry = vectorize_entry(entry)\n",
        "    dot_product = vector_entry.dot(vectors.T)\n",
        "    _, indexes, values = find(dot_product) # pax vraiment la cosine distance car les vecteurs ne sont pas normalis√©s attention\n",
        "    similar_posts_indexes = [x for _, x in sorted(zip(values, indexes), reverse=True)][:10]\n",
        "    return posts.cleaned_body.values[similar_posts_indexes]\n",
        "\n"
      ],
      "metadata": {
        "id": "X2JhLiCU1NPu"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entry = 'This is a test'"
      ],
      "metadata": {
        "id": "FHVHzd-G3Sd0"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How you can improve this approach ? "
      ],
      "metadata": {
        "id": "3AcnQSRd7XJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On peut penser √† le faire sur le titre, utiliser des ngrams, faire le cont vectorizer au niveau des lettres, utiliser un tfidf pour discriminer les mots importants"
      ],
      "metadata": {
        "id": "DqvzMfS87m3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v-TTENiM5vn2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}